import re
import copy

from . import utils

from .Dtype import Dtype
from .Kernel import Kernel
from .Ringbuf import Ringbuf


####################################################################################################


class DedisperserParams:
    def __init__(self, dtype, rank, apply_input_residual_lags, input_is_ringbuf, output_is_ringbuf, nspec):
        """
        The 'rank' arg can either be an integer (representing a single kernel),
        or a list of integers (representing a file containing multiple kernels).
        """

        dtype = Dtype(dtype)
        assert 1 <= nspec <= (512 // dtype.nbits)
        assert utils.is_power_of_two(nspec)
        assert not (input_is_ringbuf and output_is_ringbuf)
        
        max_rank = self.max_rank(dtype.nbits, nspec)

        if isinstance(rank, int):
            assert 1 <= rank <= max_rank
        elif isinstance(rank, list):
            assert len(rank) > 0
            assert all(isinstance(r,int) for r in rank)
            assert all(1 <= r <= max_rank for r in rank)
        else:
            raise RuntimeError("Expected 'rank' to be an integer, or list of integers")            
        
        self.dtype = dtype
        self.rank = rank
        self.apply_input_residual_lags = apply_input_residual_lags
        self.input_is_ringbuf = input_is_ringbuf
        self.output_is_ringbuf = output_is_ringbuf
        self.nspec = nspec


    @classmethod
    def max_rank(cls, nbits, nspec):
        # This expression for the max rank derives from the requirement that shared
        # memory lags must be at most 255 32-bit registers.
        max_rank = 13 - utils.integer_log2(nbits * nspec)
        return min(max_rank, 8)

    
    @property
    def filename(self):
        """Note: the rank is ignored (see from_filename() below)."""
        
        if (not self.input_is_ringbuf) and (not self.output_is_ringbuf):
            rb = 'norb'
        elif (self.input_is_ringbuf) and (not self.output_is_ringbuf):
            rb = 'inrb'
        elif (not self.input_is_ringbuf) and (self.output_is_ringbuf):
            rb = 'outrb'
        else:
            raise RuntimeError('should never get here')

        lag = 'rlag' if self.apply_input_residual_lags else 'nolag'
        return f'dd_{self.dtype.fname}_{rb}_{lag}_s{self.nspec}.cu'


    @classmethod
    def from_filename(cls, filename):
        """
        Example filename: 'dd_fp32_norb_nolag_s1.cu'.
        
        The filename can include an optional leading directory and a trailing '.cu'.
        For example, /path/to/dd_fp16_inrb_rlag_s8.cu matches.
        """

        rb_dict = { 'norb': (False,False), 'inrb': (True,False), 'outrb': (False,True) }   # (in_rb, out_rb)
        lag_dict = { 'nolag': False, 'rlag': True}   # rlag

        try:
            # Reminder: (?:...) defines a "non-capturing group".
            m = re.match(r'^(?:.*/)?dd_(fp\d+)_([a-z]+rb)_([a-z]+lag)_s(\d+)(?:\.cu)?', filename)
            dtype = Dtype(m.group(1))
            irb, orb = rb_dict[m.group(2)]
            rlag = lag_dict[m.group(3)]
            nspec = int(m.group(4))
        except:
            raise RuntimeError(f"cuda_generator.DedisperserParams: couldn't parse {filename=}")

        # We generate all kernels up to the max rank.
        max_rank = cls.max_rank(dtype.nbits, nspec)
        rank_list = list(range(1, max_rank+1))
        
        return cls(
            dtype = dtype,
            rank = rank_list,
            apply_input_residual_lags = rlag,
            input_is_ringbuf = irb,
            output_is_ringbuf = orb,
            nspec = nspec
        )

    
####################################################################################################


def make_dd_file(filename):
    """Returns a Kernel object."""
    
    params = DedisperserParams.from_filename(filename)
    assert isinstance(params.rank, list)

    k = Kernel()
    
    k.emit('// Autogenerated by pirate_frb.cuda_generator')
    k.emit()
    k.emit('// For a high-level overview, see comments at the top of')
    k.emit('// pirate_frb/cuda_generator/Dedisperser.py')
    k.emit()
    k.emit('#include <cstdio>')
    k.emit('#include <cassert>')
    k.emit('#include <ksgpu/device_transposes.hpp>   // f16_perm(), f16_align()')
    k.emit('#include "../../include/pirate/DedispersionKernel.hpp"')
    k.emit()
    k.emit('namespace pirate {')
    k.emit()

    k_code = k.splice()

    for rank in params.rank:
        params2 = copy.copy(params)
        params2.rank = rank
        
        dd = Dedisperser(params2)
        dd.emit_global(k_code)
        k_code.emit()

        k_code.reset_names()
        
        k.emit(f'// Boilerplate to register the rank={rank} kernel when the library is loaded.')
        k.emit('namespace {')
        k.emit(f'struct register_hack{rank} {{')
        k.emit(f'register_hack{rank}() {{')
        k.emit()

        k.emit('GpuDedispersionKernel::RegistryKey k;')
        k.emit(f'k.dtype = ksgpu::Dtype::native<{params.dtype.scalar}>();')
        k.emit(f'k.rank = {rank};')
        k.emit(f'k.nspec = {params.nspec};')
        k.emit(f'k.input_is_ringbuf = {utils.tf(params.input_is_ringbuf)};')
        k.emit(f'k.output_is_ringbuf = {utils.tf(params.output_is_ringbuf)};')
        k.emit(f'k.apply_input_residual_lags = {utils.tf(params.apply_input_residual_lags)};')
        k.emit()
        
        k.emit('GpuDedispersionKernel::RegistryValue v;')
        k.emit(f'v.shmem_nbytes = {dd.shmem_nbytes};')
        k.emit(f'v.warps_per_threadblock = {dd.warps_per_threadblock};')
        k.emit(f'v.pstate32_per_small_tree = {dd.pstate32_per_small_tree};')
        k.emit(f'v.nt_per_segment = {dd.nt_per_segment};')

        if (not params.input_is_ringbuf) and (not params.output_is_ringbuf):
            k.emit(f'v.cuda_kernel_no_rb = {dd.kernel_name};')
        elif (params.input_is_ringbuf) and (not params.output_is_ringbuf):
            k.emit(f'v.cuda_kernel_in_rb = {dd.kernel_name};')
        elif (not params.input_is_ringbuf) and (params.output_is_ringbuf):
            k.emit(f'v.cuda_kernel_out_rb = {dd.kernel_name};')
        else:
            raise RuntimeError('Error: input_is_ringbuf == output_is_ringbuf == True')

        k.emit()
        
        k.emit(f'bool debug = false;')
        k.emit(f'GpuDedispersionKernel::registry().add(k,v,debug);')
        k.emit()
    
        k.emit(f'}}   // register_hack{rank} constructor')
        k.emit(f'}};  // struct register_hack{rank}')
        k.emit(f'register_hack{rank} hack{rank};')
        k.emit('}   // anonymous namespace')
        k.emit()

    k.emit('} // namespace pirate')
    return k
    

####################################################################################################


class Dedisperser:
    def __init__(self, params):
        """
        We distinguish notationally between three ring buffers:
           - grb: global ring buffer, used if params.{input,output}_is_ringbuf == True
           - srb: shared memory ring buffer, used if self.two_stage == True
           - rrb: per-warp register ring buffer (always used).
        """
        
        assert isinstance(params, DedisperserParams)
        assert isinstance(params.rank, int)
        
        self.params = params
        self.dtype = params.dtype
        self.dt32 = params.dtype.simd32
        self.nbits = params.dtype.nbits
        self.rank = params.rank
        self.apply_input_residual_lags = params.apply_input_residual_lags
        self.input_is_ringbuf = params.input_is_ringbuf
        self.output_is_ringbuf = params.output_is_ringbuf
        self.nt_per_segment = utils.xdiv(1024, self.nbits * params.nspec)
        self.nspec = params.nspec
        
        self.kernel_name = f'dd_fp{self.nbits}'
        self.kernel_name += f'_r{params.rank}'
        self.kernel_name += f'_ilag{int(params.apply_input_residual_lags)}'
        self.kernel_name += f'_irb{int(params.input_is_ringbuf)}'
        self.kernel_name += f'_orb{int(params.output_is_ringbuf)}'
        self.kernel_name += f'_s{params.nspec}'
        
        self.rrb = Ringbuf(self.dt32)   # simd dtype, not scalar dtype

        # This simple rule works and gives good performance, for a reasonable
        # range of 'nspec' values.
        self.two_stage = (params.rank >= 3)

        if self.two_stage:
            self.rank0 = (params.rank // 2)
            self.rank1 = params.rank - self.rank0
            self.warps_per_threadblock = 2**self.rank0
            self.shmem_nbytes = 4 * self.srb_base(2**self.rank0, 0)
            self.ndd = 2**(self.rank1)   # dedispersion "data" registers per thread
            self.static_asserts()
        else:
            # FIXME single-stage kernel should use 4 warps/threadblock
            self.warps_per_threadblock = 1
            self.shmem_nbytes = 0
            self.ndd = 2**(self.rank)    # dedispersion "data" registers per thread

    
    def emit_global(self, k):
        assert isinstance(k, Kernel)
        
        W = self.warps_per_threadblock
        B = utils.xdiv(16, W)   # threadblocks per SM (FIXME rethink?)

        k.emit(f'// Launch with {{ 32, {W} }} threads/warp')
        k.emit(f'// Launch with {{ Namb, Nbeams }} threadblocks')
        k.emit()

        if False:   # debug
            self._show_shmem_layout_in_comment(k)

        # Kernel args depend on whether input/output is a ring buffer.
        rb_args = 'void *grb_base_, uint *grb_loc_, long grb_pos'
        in_args = rb_args if self.input_is_ringbuf else 'void *inbuf_, long beam_istride32, int amb_istride32, int act_istride32'
        out_args = rb_args if self.output_is_ringbuf else 'void *outbuf_, long beam_ostride32, int amb_ostride32, int act_ostride32'
        
        k.emit(f'__global__ void __launch_bounds__({32*W},{B})')
        k.emit(f'{self.kernel_name}({in_args},')
        k.emit(f'       {out_args},')
        k.emit(f'       void *pstate_, int ntime, ulong nt_cumul, bool is_downsampled_tree)')
        k.emit('{')

        self._apply_inbuf_offsets(k)    # behaves differently, depending on self.input_is_ringbuf
        self._apply_outbuf_offsets(k)   # behaves differently, depending on self.output_is_ringbuf
        self._init_srb(k)               # no-ops if (self.two_stage) is False

        # Save splice, for code to load pstate.
        # This code must be emitted near the end, after the ring buffer layout is finalized.
        k_pstate = k.splice()
        
        k.emit(f'for (int itime = 0; itime < ntime; itime += {self.nt_per_segment}) {{')

        self._load_input_data(k)              # behaves differently, depending on self.input_is_ringbuf
        self._apply_input_residual_lags(k)    # no-ops if (self.apply_input_residual_lags) is False
        self._dedispersion_core(k)            # behaves differently, depending on self.two_stage
        self._save_output_data(k)             # behaves differently, depending on self.output_is_ringbuf

        self._advance_rrb(k)
        self._advance_srb(k)      # no-ops if (self.two_stage) is False
        self._advance_inbuf(k)    # behaves differently, depending on self.input_is_ringbuf
        self._advance_outbuf(k)   # behaves differently, depending on self.output_is_ringbuf
        
        k.emit('}   // outer time loop')

        # Now that 'self.rrb' has been finalized, we can sort out the pstate.
        self._lay_out_pstate(k_pstate)    # initializes some members, including self.pstate32_per_small_tree
        self._load_pstate(k_pstate)
        self._save_pstate(k)
        
        k.emit('}   // kernel')

        
    def _load_input_data(self, k):
        if self.input_is_ringbuf:
            k.emit('// Load data from input ringbuf into registers.')
            k.emit('// Reminder: grb_loc is indexed by (tseg, dm_amb, f)')
            k.emit('// FIXME: terrible parallelization in this step (all threads do same computation!)')

            for i in range(self.ndd):
                ix = self._grb_ix(k, i, 0)
                k.emit(f'{self.dt32} dd{i} = grb_base[{ix}];')

        else:
            k.emit('// Load data from global memory into registers')
            for i in range(self.ndd):
                s = f'{i} * act_istride32' if (i > 0) else '0'
                k.emit(f'{self.dt32} dd{i} = inbuf[{s}];')
        
        k.emit()
        

    def _save_output_data(self, k):
        # To convert a register index to a dmbr index, apply this shift.
        dshift = self.rank0 if self.two_stage else 0

        if self.output_is_ringbuf:
            k.emit('\n//Store data from registers to output ringbuf')
            k.emit('// Reminder: grb_loc is indexed by (tseg, f, dmbr)')
            k.emit('// FIXME: terrible parallelization in this step (all threads do same computation!)')

            for i in range(self.ndd):
                ix = self._grb_ix(k, i, dshift)
                k.emit(f'grb_base[{ix}] = dd{i};')

        else:
            k.emit('\n//Store data from registers to global memory')
            for i in range(self.ndd):
                k.emit(f'outbuf[({i} * act_ostride32) << {dshift}] = dd{i};')
        
        k.emit()


    def _grb_ix(self, k, i, dshift):
        """Helper, called by _load_input_data() and _save_output_data()."""
        
        q, rb_offset, rb_phase, rb_len, rb_nseg, ix = k.get_tmp_rname(6)
                
        k.emit(f'uint4 {q} = grb_loc[{i} << {dshift}];')
        k.emit(f'uint {rb_offset} = {q}.x;  // rb_offset, in segments not bytes')
        k.emit(f'uint {rb_phase} = {q}.y;   // rb_phase: index of (time chunk, beam) pair, relative to current pair')
        k.emit(f'uint {rb_len} = {q}.z;     // rb_len: number of (time chunk, beam) pairs in ringbuf (same as Ringbuf::rb_len)')
        k.emit(f'uint {rb_nseg} = {q}.w;    // rb_nseg: number of segments per (time chunk, beam)')
        k.emit(f'{rb_phase} = (grb_pos + {rb_phase}) % {rb_len};   // updated rb_phase')
        k.emit(f'{rb_offset} += ({rb_phase} * {rb_nseg});      // updated rb_offset')
        k.emit(f'long {ix} = (long({rb_offset}) << 5) + threadIdx.x;  // index relative to rb_base')
        return ix
    
    
    def _dedispersion_core(self, k):
        if self.two_stage:
            self._two_stage_dedispersion_core(k)
        else:
            self._single_stage_dedispersion_core(k)

    
    def _single_stage_dedispersion_core(self, k):
        for i in range(self.rank):
            self._dedispersion_pass(k, i)

        
    def _two_stage_dedispersion_core(self, k):
        for i in range(self.rank0):
            self._dedispersion_pass(k, i)

        k.emit()
        k.emit('__syncthreads();')
        k.emit()
        k.emit('// Write dd registers to shared memory')

        for i in range(self.ndd):
            s = self._srb_loc(k, i)
            k.emit(f'shmem[{s}] = dd{i};')

        k.emit()
        k.emit('__syncthreads();')
        k.emit()
            
        k.emit('// Read dd registers from shared memory')

        for i in range(self.ndd):
            s = self._srb_loc(k, 16+i)
            k.emit(f'dd{i} = shmem[{s}];')

        # One-sample lags are needed in the float16 case.
        self._apply_one_sample_lags(k)
        
        for i in range(self.rank1):
            self._dedispersion_pass(k, i)

    
    def _dedispersion_pass(self, k, i):
        assert 0 <= i < utils.integer_log2(self.ndd)

        k.emit()
        k.emit(f'// dedispersion_pass: pass {i} starts here')
        
        # Outer loop is a spectator index.
        for s in range(0, self.ndd, 2**(i+1)):
            for j in range(2**i):
                x0 = f'dd{s+j}'
                x1 = f'dd{s+j+2**i}'
                lag = utils.bit_reverse(j,i)
                
                k.emit(f'// dedisperse {x0}, {x1} with time lag {lag} (not accounting for nbits, nspec)')
                tmp0, tmp1 = self._advance2(k, x0, lag)
                k.emit(f'{x0} = {tmp1} + {x1};')
                k.emit(f'{x1} += {tmp0};')

        k.emit(f'// dedispersion_pass: pass {i} ends here')


    def _advance2(self, k, x0, lag):
        """Lag is in time samples, and does not account for nbits, nspec."""
        
        lag32a = (lag * self.nbits * self.nspec) // 32
        lag32b = (self.nbits * self.nspec) // 32

        if lag32b > 0:
            # Case 1: one time sample corresponds to an integer number of 32-bit registers.
            tmp0, tmp1 = self.rrb.advance2(k, x0, lag32a, lag32b)
        else:
            # Case 2 (contrary case): should only get here if dtype==float16 and nspec==1.
            assert (self.dtype.scalar == '__half') and (self.nspec == 1)
            tmp0, tmp1 = self.rrb.advance2(k, x0, lag32a, 1)
            tdst = tmp1 if (lag % 2) else tmp0
            k.emit(f'{tdst} = ksgpu::f16_align({tmp0}, {tmp1});')
        
        return tmp0, tmp1
        

    def _apply_input_residual_lags(self, k):
        if not self.apply_input_residual_lags:
            return

        k.emit(f'// Apply input "residual" lags.')
        k.emit(f'//')
        k.emit(f'// Context: consider the second stage in a two-stage dedisperser.')
        k.emit(f'// The ambient index corresponds to a bit-reversed DM 0 <= dm_brev < 2^(ambient_rank).')
        k.emit(f'// Each input register corresponds to a coarse frequency 0 <= f < 2^(dd_rank).')
        k.emit(f'//')
        k.emit(f'// Between the two stages of the tree, we want to apply the following lag:')
        k.emit(f'//    lag = dm * frev')
        k.emit(f'//')
        k.emit(f'// where:')
        k.emit(f'//    dm = bit_reverse(dm_brev) + (is_downsampled_tree ? 2^ambient_rank : 0)')
        k.emit(f'//    frev = 2^dd_rank - 1 - f')
        k.emit(f'//')
        k.emit(f'// However, due to cache-alignment constraints, we are only able to apply (lag % T),')
        k.emit(f'// where T = self.nt_per_segment.  To address this, the dedispersion kernel is')
        k.emit(f'// responsible for applying the "residual" lag:')
        k.emit(f'//    rlag = lag % T')
        k.emit(f'//')
        k.emit(f'// FIXME: poorly optimized, especially in float16 case')
        k.emit()
        
        T = self.nt_per_segment
        B = self.nspec * self.nbits
        assert T*B == 1024
        
        k.emit(f'uint rlag_f0 = threadIdx.y << {utils.integer_log2(self.ndd)};         // 0 <= rlag_f0 < 2^(dd_rank)')
        k.emit(f'uint rlag_dm = __brev(blockIdx.x) >> (33U - __ffs(gridDim.x));   // 0 <= rlag_dm < 2^(ambient rank)')
        k.emit(f'rlag_dm += (is_downsampled_tree ? gridDim.x : 0);')

        for i in range(self.ndd):
            rlag = k.get_tmp_rname()
            k.emit(f'uint {rlag} = (rlag_dm * ({2**self.rank-1-i} - rlag_f0)) & {T-1};   // rlag to be applied to dd{i}')

            if (B % 32) == 0:
                # Case 1: one time sample corresponds to an integer number of 32-bit registers.
                if B > 32:
                    k.emit(f'{rlag} <<= {utils.integer_log2(B//32)};   // convert rlag (time samples) -> (32-bit registers)')
                
                tmp = k.get_tmp_rname()
                k.emit(f'{self.dt32} {tmp} = ((threadIdx.x + {rlag}) >= 32) ? dd_rlag{i} : dd{i};')
                k.emit(f'dd_rlag{i} = dd{i};')
                k.emit(f'dd{i} = __shfl_sync(0xffffffff, {tmp}, threadIdx.x - {rlag});   // rlag has now been applied to dd{i}')

            else:
                # Case 2 (contrary case): should only get here if dtype==float16 and nspec==1.
                assert (self.nbits == 16) and (self.nspec == 1)
                rlag2, tmp, tmp2 = k.get_tmp_rname(3)
                k.emit(f'uint {rlag2} = {rlag} >> 1;   // (rlag // 2)')

                k.emit(f'if ({rlag} & 1) {{')
                k.emit(f'    {self.dt32} {tmp} = ((threadIdx.x + {rlag2} + 1) >= 32) ? dd_rlag{i} : dd{i};')
                k.emit(f'    {self.dt32} {tmp2} = ((threadIdx.x + {rlag2}) >= 32) ? dd_rlag{i} : dd{i};')
                k.emit(f'    dd_rlag{i} = dd{i};')
                k.emit(f'    {tmp} = __shfl_sync(0xffffffff, {tmp}, threadIdx.x - {rlag2} - 1);')
                k.emit(f'    {tmp2} = __shfl_sync(0xffffffff, {tmp2}, threadIdx.x - {rlag2});')
                k.emit(f'    dd{i} = ksgpu::f16_align({tmp}, {tmp2});')
                k.emit(f'}}')
                k.emit(f'else {{')
                k.emit(f'    {self.dt32} {tmp} = ((threadIdx.x + {rlag2}) >= 32) ? dd_rlag{i} : dd{i};')
                k.emit(f'    dd_rlag{i} = dd{i};')
                k.emit(f'    dd{i} = __shfl_sync(0xffffffff, {tmp}, threadIdx.x - {rlag2});')
                k.emit(f'}}     // rlag has now been applied to dd{i}')

    
    def _apply_one_sample_lags(self, k):
        if (self.nbits != 16) or (self.nspec != 1) or (self.rank0 == 0):
            return

        k.emit(f'// Apply one-sample lags (float16 nspec=1 only)')
        k.emit(f'// A one-sample lag is needed if:')
        k.emit(f'//    - register index is even (equivalent to odd frev)')
        k.emit(f'//    - (warpId & {2**(self.rank0-1)}) is 1 (equivalent to odd dm)')
        k.emit(f'//')
        k.emit(f'// FIXME: suboptimal approach, should coalesce with dd1')
        k.emit()
        
        sel = k.get_tmp_rname()
        k.emit(f'uint {sel} = (threadIdx.y & {2**(self.rank0-1)}) ? 0x5432 : 0x7654;')

        for i in range(0, self.ndd, 2):  # even registers
            xprev = k.get_tmp_rname()
            k.emit(f'__half2 {xprev};   // will hold result of Ringbuf.advance(dd{i},1)')
            self.rrb.advance(k, f'dd{i}', 1, dst=xprev)
            k.emit(f'dd{i} = ksgpu::f16_perm({xprev}, dd{i}, {sel});  // dd{i} has now been lagged by one sample, if needed')
        
        k.emit()
        

    def _apply_inbuf_offsets(self, k):
        if self.input_is_ringbuf:
            wshift = self.rank1 if self.two_stage else self.rank
            self._apply_grb_offsets(k, wshift)
            return
        
        # Non-ringbuf case follows.
        k.emit(f'// Apply per-thread offsets to inbuf (including laneId offset).')
        k.emit(f'{self.dt32} *inbuf = ({self.dt32} *) inbuf_;')
        k.emit(f'inbuf += long(beam_istride32) * long(blockIdx.y);   // beam = blockIdx.y')
        k.emit(f'inbuf += long(amb_istride32) * long(blockIdx.x);    // ambient = blockIdx.x')
        
        if self.two_stage:
            k.emit(f'inbuf += {2**self.rank1} * long(act_istride32) * long(threadIdx.y);   // warpId = threadIdx.y')
            
        k.emit(f'inbuf += threadIdx.x;   // laneId')
        k.emit()


    def _apply_outbuf_offsets(self, k):
        if self.output_is_ringbuf:
            self._apply_grb_offsets(k, 0)
            return

        # Non-ringbuf case follows.
        k.emit(f'// Apply per-thread offsets to outbuf (including laneId offset).')
        k.emit(f'{self.dt32} *outbuf = ({self.dt32} *) outbuf_;')
        k.emit(f'outbuf += long(beam_ostride32) * long(blockIdx.y);   // beam = blockIdx.y')
        k.emit(f'outbuf += long(amb_ostride32) * long(blockIdx.x);    // ambient = blockIdx.x')
        
        if self.two_stage:
            k.emit(f'outbuf += long(act_ostride32) * long(threadIdx.y);   // warpId = threadIdx.y')
            
        k.emit(f'outbuf += threadIdx.x;   // laneId')
        k.emit()


    def _apply_grb_offsets(self, k, wshift):
        wshift = f' << {wshift}' if (wshift > 0) else ''
        
        k.emit(f'// Apply per-thread offsets to global memory ring buffer')
        k.emit(f'// Note: grb_loc is indexed by (tseg, amb, active_ix)')
        k.emit(f'// where active_ix = (freq for input ringbuf, or dmbr for output ringbuf)')

        k.emit(f'{self.dt32} *grb_base = ({self.dt32} *) grb_base_;')
        k.emit(f'uint4 *grb_loc = (uint4 *) (grb_loc_);')
        k.emit(f'grb_loc += (blockIdx.x << {self.rank});   // dm_amb = blockIdx.x')
        k.emit(f'grb_loc += threadIdx.y{wshift};   // warpId')
        k.emit(f'grb_pos += blockIdx.y;    // beam = blockIdx.y')
        k.emit()

    
    def _init_srb(self, k):
        if not self.two_stage:
            return
        
        rank0, rank1 = self.rank0, self.rank1
        B = self.nspec * self.nbits
        
        k.emit('// Init control words for two-stage kernel.')
        k.emit('// A ring buffer "control word" consists of:')
        k.emit('//')
        k.emit('//   uint15 srb_base;   // base shared memory location of ring buffer (in 32-bit registers)')
        k.emit('//   uint9  srb_pos;    // current position, satisfying 0 <= srb_pos < (srb_lag + 32)')
        k.emit('//   uint8  srb_lag;    // ring buffer lag (in 32-bit registers), note that capacity = lag + 32.')
        k.emit('//')
        k.emit('// Depending on context, srb_pos may point to either the end of the buffer')
        k.emit('// (writer thread context), or be appropriately lagged (reader thread context).')
        k.emit('//')
        k.emit(f'// Lanes [0:{2**rank1}] hold control words for registers in the first pass:')
        k.emit(f'//')
        k.emit(f'//   - registers form a ({2**(rank1-rank0)}, {2**rank0}) array, where first index is a "fast"')
        k.emit(f'//     frequency 0 <= ff < {2**(rank1-rank0)} and second index is 0 <= dm_brev < {2**rank0}')
        k.emit(f'//   - warpId is a "slow" frequency 0 <= fs < {2**rank0}')
        k.emit(f'//   - writer thread context')
        k.emit(f'//')
        k.emit(f'// Lanes [16:16+{2**rank1}] hold control words for registers in the second pass:')
        k.emit(f'//')
        k.emit(f'//   - register index is a frequency 0 <= f < {2**rank1})')
        k.emit(f'//   - warpId is 0 <= dm_brev < {2**rank0}')
        k.emit(f'//   - reader thread context')
        k.emit()

        first_pass, dm_brev, dm, f, frev = k.get_tmp_rname(5)

        dr = rank1 - rank0
        fpass1 = 'threadIdx.y' if (dr==0) else f'(threadIdx.y << {dr}) | ((threadIdx.x >> {rank0}) & {2**dr-1})'

        k.emit(f'// Compute frev, dm.')
        k.emit(f'bool {first_pass} = (threadIdx.x < 16);   // first_pass')
        k.emit(f'uint {dm_brev} = {first_pass} ? (threadIdx.x & {2**rank0-1}) : threadIdx.y;   // dm_brev')
        k.emit(f'uint {dm} = __brev({dm_brev}) >> {32-rank0};   // dm')
        k.emit(f'uint {f} = {first_pass} ? ({fpass1}) : (threadIdx.x & {2**rank1-1});  // f')
        k.emit(f'uint {frev} = {2**rank1-1} - {f};   // frev')
        k.emit()

        k.emit(f'// Compute (srb_lag, srb_base) from (frev, dm).')
        k.emit(f'// This part uses some cryptic, highly optimized code from Dedisperser.srb_base()')

        if (B % 32) == 0:
            # Case 1: one time sample corresponds to an integer number of 32-bit registers.
            a = (B//32) * 2**(rank1-1) * (2**rank1-1)
            b = 2**(rank1+6) - a
            
            t, srb_base, srb_lag = k.get_tmp_rname(3)
            
            k.emit(f'uint {t} = {frev}*({frev}-1);')
            if B > 32:
                k.emit(f'{t} <<= {utils.integer_log2(B//32)};')

            k.emit(f'{t} += {a}*{dm} + {b};')
            k.emit(f'uint {srb_base} = (({dm}*{t}) >> 1) + ({frev} << 5);  // srb_base (32-bit registers, not time samples)')

            k.emit(f'uint {srb_lag} = {dm}*{frev};  // srb_lag')
            if B > 32:
                k.emit(f'{srb_lag} <<= {utils.integer_log2(B//32)};  // convert srb_lag (time samples) -> (32-bit registers)')
                
        else:
            # Case 2 (contrary case): should only get here if dtype==float16 and nspec==1.
            assert (self.dtype.scalar == '__half') and (self.nspec == 1)
            a = (2**rank1 - 1) * 2**(rank1-1)
            b = (256 - 2**rank1) * 2**(rank1-1)
            c = 2**(rank1-1) + 1

            u, t, srb_base, srb_lag = k.get_tmp_rname(4)
            k.emit(f'uint {u} = {a}*{dm} + {frev}*({frev}-1) + {b};')
            k.emit(f'uint {t} = ({dm}*{u}) + ({dm} & 1) * ({c}-{frev});')
            k.emit(f'uint {srb_base} = ({t} >> 2) + ({frev} << 5);  // srb_base')
            k.emit(f'uint {srb_lag} = ({dm}*{frev}) >> 1;  // srb_lag')

        k.emit()
        k.emit(f'// Compute srb_pos, and assemble control word from (srb_lag, srb_base)')
        k.emit(f'// Note: srb_pos = 32 * (nt_cumul / nt_per_segment), where nt_per_segment = {self.nt_per_segment}')
        
        # Left shift needed for (nt_cumul) -> (nt_per_segment), can be negative.
        ls = 5 - utils.integer_log2(self.nt_per_segment)

        srb_pos = k.get_tmp_rname()
        if ls > 0:
            k.emit(f'uint {srb_pos} = nt_cumul << {ls};   // srb_pos')
        elif ls < 0:
            k.emit(f'uint {srb_pos} = nt_cumul >> {-ls};  // srb_pos')
        else:
            k.emit(f'uint {srb_pos} = nt_cumul;  // srb_pos')

        k.emit(f'{srb_pos} += ({first_pass} ? 0 : 32);   // srb_pos: add reader/writer offset')
        k.emit(f'{srb_pos} %= ({srb_lag} + 32U);         // srb_pos: mod (srb_lag + 32)')
        k.emit(f'uint control_word = {srb_base} | ({srb_pos} << 15) | ({srb_lag} << 24);')
        k.emit()


    def _srb_loc(self, k, control_lane):
        w, srb_base, srb_pos, srb_size, wraparound, ret = k.get_tmp_rname(6)
        k.emit(f'uint {w} = __shfl_sync(0xffffffff, control_word, {control_lane});  // srb_loc({control_lane=}) starts here')
        k.emit(f'uint {srb_base} = ({w} & 0x7fff);        // srb_base, 15 bits')
        k.emit(f'uint {srb_pos} = (({w} >> 15) & 0x1ff) + threadIdx.x;   // srb_pos, 9 bits (note laneId at end)')
        k.emit(f'uint {srb_size} = ({w} >> 24) + 32;      // srb_size, 8 bits (note +32 here to convert srb_lag -> srb_size')
        k.emit(f'uint {wraparound} = ({srb_pos} >= {srb_size}) ? {srb_size} : 0;  // wraparound')
        k.emit(f'uint {ret} = {srb_base} + {srb_pos} - {wraparound};  // srb_loc({control_lane=}) return value')
        return ret


    def _advance_rrb(self, k):
        k.emit('// Advance in-register ring buffer for next iteration of loop')
        self.rrb.advance_outer(k)
        k.emit()

    
    def _advance_srb(self, k):
        if not self.two_stage:
            return

        pos15, lag15, dpos15 = k.get_tmp_rname(3)
        
        k.emit(f'// Advance control words (for shared memory ring buffer)')
        k.emit(f'int {pos15} = (control_word & 0xff8000);  // pos15')
        k.emit(f'int {lag15} = ((control_word >> 9) & 0xff8000);  // lag15')
        k.emit(f'int {dpos15} = ({pos15} >= {lag15}) ? {lag15} : (-(32 << 15));  // dpos15')
        k.emit(f'control_word = (control_word & 0xff007fff) | ({pos15} - {dpos15});')
        k.emit()

    
    def _advance_inbuf(self, k):
        if self.input_is_ringbuf:
            self._advance_grb(k)
        else:
            k.emit('// Advance inbuf pointer')
            k.emit('inbuf += 32;')
            k.emit()
    

    def _advance_outbuf(self, k):
        if self.output_is_ringbuf:
            self._advance_grb(k)
        else:
            k.emit('// Advance outbuf pointer')
            k.emit('outbuf += 32;')
            k.emit()

    
    def _advance_grb(self, k):
        k.emit('// Advance global memory ring buffer')
        k.emit(f'grb_loc += (gridDim.x << {self.rank});   // nambient = gridDim.x')
        k.emit()
        

    def _lay_out_pstate(self, k):
        RL32 = (32*self.ndd) if self.apply_input_residual_lags else 0    # registers for apply_input_residual_lags, per warp
        RB32 = self.rrb.get_n32_per_warp()                               # registers for in-register ringbuf, per warp
        SM32 = utils.xdiv(self.shmem_nbytes,4)                           # shared memory 32-bit words, per threadblock
        W = self.warps_per_threadblock
        
        k.emit(f'// Per-threadblock persistent state layout is {self.dt32}[PS32], broken down as follows:')
        k.emit(f'//    {self.dt32} srb[SM32];')
        k.emit(f'//    {self.dt32} rrb[{W}][RL32+RB32];  // outer index is warp id')
        k.emit(f'//')
        k.emit(f'// where definitions of PS32, SM32, RL32, RB32 follow.')
        k.emit()
        k.emit(f'constexpr int {RL32 = };    // 32-bit elements per **warp** needed for apply_input_residual_lags')
        k.emit(f'constexpr int {RB32 = };    // 32-bit elements per **warp**, in **register** ring buffer')
        k.emit(f'constexpr int {SM32 = };  // 32-bit elements per **threadblock**, in **shmem** ring buffer')
        k.emit(f'constexpr int PS32 = SM32 + {W}*(RL32+RB32);   // total persistent state per **threadblock**')
        k.emit()
        
        self.pstate32_per_small_tree = SM32 + W*(RL32+RB32)

        k.emit("// Apply per-block offset to pstate")
        k.emit(f'int blockId = (gridDim.x * blockIdx.y) + blockIdx.x;')
        k.emit(f'{self.dt32} *pstate = ({self.dt32} *)pstate_;')
        k.emit(f"pstate += blockId * PS32;")
        k.emit()

        
    def _load_pstate(self, k):
        if self.two_stage:
            k.emit(f'extern __shared__ {self.dt32} shmem[];')
            k.emit()
            k.emit(f'// Load shmem ring buffer from global memory')
            k.emit(f'// NOTE no __syncthreads() here')
            k.emit(f'for (int s = 32*threadIdx.y + threadIdx.x; s < SM32; s += {32*self.warps_per_threadblock})')
            k.emit(f'    shmem[s] = pstate[s];')
            k.emit('__syncwarp();')
            k.emit()

        wp = self._warp_pstate1(k)
        
        if self.apply_input_residual_lags:
            k.emit('// Load persistent state for apply_input_residual_lags')
            for i in range(self.ndd):
                i32 = f' + {32*i}' if (i > 0) else ''
                k.emit(f'{self.dt32} dd_rlag{i} = {wp}[threadIdx.x{i32}];')
            
            k.emit()
            wp = self._warp_pstate2(k, wp)
        
        k.emit('// Read register ring buffer directly from global memory.')
        k.emit('// FIXME: would it be better to go through shared memory?')

        self.rrb.initialize(k, wp)
        k.emit()
        

    def _save_pstate(self, k):
        if self.two_stage:
            k.emit(f'// Write shmem ring buffer to global memory')
            k.emit(f'for (int s = 32*threadIdx.y + threadIdx.x; s < SM32; s += {32*self.warps_per_threadblock})')
            k.emit(f'    pstate[s] = shmem[s];')
            k.emit(f'__syncwarp();')
            k.emit()
        
        wp = self._warp_pstate1(k)
        
        if self.apply_input_residual_lags:
            k.emit('// Save persistent state for apply_input_residual_lags')
            for i in range(self.ndd):
                i32 = f' + {32*i}' if (i > 0) else ''
                k.emit(f'{wp}[threadIdx.x{i32}] = dd_rlag{i};')

            k.emit()
            wp = self._warp_pstate2(k, wp)
            
        k.emit('// Write register ring buffer to global memory.')
        k.emit('// FIXME: would it be better to go through shared memory?')
        k.emit()
        
        self.rrb.finalize(k, wp)
        k.emit()


    def _warp_pstate1(self, k):
        if (self.shmem_nbytes == 0) and (self.warps_per_threadblock == 1):
            return 'pstate'
        
        wp = k.get_tmp_rname()
        k.emit(f'// Per-warp ring buffer in global memory (length RL32+RB32)')
        k.emit(f'{self.dt32} *{wp} = pstate + SM32 + (threadIdx.y * (RL32+RB32));')
        k.emit()
        
        return wp

    
    def _warp_pstate2(self, k, wp):
        wp2 = k.get_tmp_rname()
        k.emit(f'// Per-warp ring buffer in global memory, second part (length RB32)')
        k.emit(f'{self.dt32} *{wp2} = {wp} + RL32;')
        k.emit()
        
        return wp2


    def _show_shmem_layout_in_comment(self, k):
        if not self.two_stage:
            return
        
        for dm in range(2**self.rank0):
            for frev in range(2**self.rank1):
                srb_lag = (frev * dm * self.nbits) // 32
                srb_size = srb_lag + 32
                srb_base = self.srb_base(dm, frev)
                k.emit(f'//   {dm=} {frev=}: {srb_lag=} {srb_size=} {srb_base=}')
        k.emit()
    
    
    def static_asserts(self):
        assert self.two_stage
        expected_pos = 0
        
        for dm in range(2**self.rank0):
            for frev in range(2**self.rank1):
                assert self.srb_base(dm,frev) == expected_pos
                lag32 = (frev * dm * self.nspec * self.nbits) // 32
                
                assert lag32 <= 255   # assumed in srb "control word" layout above
                expected_pos += (lag32 + 32)

        assert self.shmem_nbytes == (expected_pos * 4)
        # print(f'static_asserts(rank0={self.rank0}, rank1={self.rank1}): pass')
        

    def srb_base(self, dm, frev):
        """Called by static_asserts(). Also called directly in constructor for shmem_nbytes."""

        assert self.two_stage
        rank0, rank1 = self.rank0, self.rank1
        assert 0 <= dm <= 2**rank0   # note <=
        assert 0 <= frev < 2**rank1  # note <
        assert rank1 >= 2            # assumed below as noted

        B = self.nspec * self.nbits

        if (B % 32) == 0:
            # Case 1: one time sample corresponds to an integer number of 32-bit registers.
            
            # "Intuitive form"
            # n1 = 2**rank1
            # term1 = ((dm*(dm-1))//2) * ((n1*(n1-1))//2)   # d < dm, all f
            # term2 = dm * ((frev*(frev-1)) // 2)           # d == dm, f < frev
            # term3 = 32 * (dm*n1 + frev)                   # 32-bit padding from previous (dm,f)
            # return (term1 + term2) * (B//32) + term3
            
            # "Fast form" with slightly fewer ops (assumes rank1 >= 1)
            a = (B//32) * 2**(rank1-1) * (2**rank1-1)  # known at compile time
            b = 2**(rank1+6) - a                       # known at compile time
            t = (frev*(frev-1)) << utils.integer_log2(B//32)
            t += a*dm + b
            return ((dm*t) >> 1) + (frev << 5)

        else:
            # Case 2 (contrary case): should only get here if dtype==float16 and nspec==1.
            assert (self.nbits == 16) and (self.nspec == 1)

            # "Intuitive form" (assumes rank1 >= 2)
            # n1 = 2**rank1
            # term1 = ((dm*(dm-1))//2) * ((n1*(n1-1)) // 4)    # d < dm, all f
            # term1 -= (dm//2) * (n1//4)                       # corrections from odd d
            # term2 = (dm * frev * (frev-1)) // 4              # d == dm, f < frev
            # term2 -= (dm & 1) * (frev // 4)                  # correction if dm is odd
            # term3 = 32 * (dm*n1 + frev)
            # return (term1 + term2 + term3) * (B//32)

            # "Fast form" with slightly fewer ops (assumes rank1 >= 2)
            a = (2**rank1 - 1) * 2**(rank1-1)     # known at compile time
            b = (256 - 2**rank1) * 2**(rank1-1)   # known at compile time
            c = 2**(rank1-1) + 1                  # known at compile time
            u = a*dm + frev*(frev-1) + b
            t = (dm*u) + (dm & 1) * (c-frev)
            return (t >> 2) + (frev << 5)

