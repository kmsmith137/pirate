# Pirate Project Rules

This file was originally written for LLMs, but isn't a bad reference for human collaborators.
(Humans can ignore content toward the end of the file.)

## Project Overview

Pirate is a CUDA/C++ pipeline for Fast Radio Burst (FRB) detection, including:
- GPU-accelerated dedispersion kernels
- Python code generators that produce optimized CUDA kernels
- High-level python interface (`pirate_frb` module) via pybind11

Uses the `chord` branch of the ksgpu library for the Array class and other helpers.
Source code for ksgpu can be found in the cursor workspace (in a "sibling" directory to pirate).
Please complain if you can't see it.

## Build System
- Uses pipmake, a tiny build system which is pip-compatible, but just forwards pip commands to a Makefile (e.g. pip install forwards to make wheel).
- The Makefile runs the script makefile_helper.py, which contains miscellaneous logic that's more convenient to write in python than in Makefile language. (For example, figuring out which directory the numpy *.h files are in.) The output of this script is a file makefile_helper.out, containing variable declarations in Makefile language.
- You can freely mix invokations of pip and make, which is convenient but can be confusing. See README.md for a recommendation.
- Compile time for the whole project is fairly high, so suggest building specific targets when developing/debugging, and build with `make -j 32`.
- The file `sdist_files.txt` is a Makefile target, and is a list of all source files (in any language). Here is a convenient way to search all source files for symbol FOO: `make sdist_files.txt; cat sdist_files.txt | xargs grep FOO`.
- If you add a new source file, see comments near the top of `Makefile` for instructions on how to modify the makefile.

## File Structure

```
include/pirate/*.hpp          - Public C++ headers
src_lib/*.cu                  - CUDA/C++ implementations
src_lib/autogenerated_kernels/*.cu  - Generated kernels (DO NOT EDIT)
pirate_frb/cuda_generator/*.py      - Python code that generates CUDA kernels
pirate_frb/*.py               - High-level python interface 
bin/                          - Compiled executables
lib/libpirate.so              - Shared library
```

## Autogenerated kernels

 - Source files of the form `src_lib/autogenerated_kernels/*.cu` are generated during the build process.
 - The Makefile runs the script `makefile_helper.py`, which writes `*.cu` filenames to `makefile_heler.out`. Each file is autogenerated with `python autogenerate_kernel.py filename.cu`.
 - Individual source files can be generated or compiled with `make src_lib/autogenerated_kernels/filename.{cu,o}`.
 - Each source file "registers" its precompiled kernel at library init time, and the registered kernels are found at runtime. (See examples in `PeakFindingKernel.hpp`.)

## Code Conventions

### Chunks, batches, frames, and segments

Throughout the code:
- A "chunk" (or "time chunk") is a range of time indices. The chunk size (e.g. 1024 or 2048) is defined in `DedispersionConfig::time_samples_per_chunk`.
- A "batch" (or "beam batch") is a range of beam indices. The batch size (e.g. 1,2,4) is defined in `DedispersionConfig::beams_per_batch`.
- A "frame" is a (chunk,beam) pair (not a (chunk,batch) pair!). Frames are used in `class MegaRingbuf`, and will also be used in the front-end server code and its intensity ring buffer.
- A "segment" refers to a 128-byte, memory-contiguous subset of any array in GPU memory. Segments are used in low-level GPU kernels, and data structures which are GPU kernel adjacent (e.g. `DedispersionPlan`, `MegaRingbuf`).

### Low-level building blocks from ksgpu

The following building blocks from the `ksgpu` library are used frequently:
 - `Array<T>` and `Array<void>`: N-dimensional array, on CPU or GPU.
 - `class Dtype`: for types determined dynamically at runtime.
 - `af_*`: memory allocation flags.
 - `xassert*`: assert macros that throw exceptions and print debugging info.
 - Wrapper functions and RAII classes for the cuda library.

If you're reading this, then you should also read `ksgpu/.cursor/rules.md` for
an overview of these building blocks.

### Helper Functions (from pirate/inlines.hpp)
```cpp
xdiv(m, n)              // divide with assertion that n divides m
pow2(n)                 // returns 2^n as long
is_power_of_two(n)      // returns bool
integer_log2(n)         // returns log2 of power-of-two
```

### Namespace Pattern
```cpp
namespace pirate {
#if 0
}  // editor auto-indent
#endif

// ... code here ...

}  // namespace pirate
```

## Style Guidelines

- Prefer explicit types over `auto` (except for iterators and complex template types)
- Prefer private (or static) functions over anonymous scopes `{ ... }`.
- Use `//` comments, not `/* */`
- Document array shapes in comments: `// shape (B, D, M, T)`
- Keep lines under ~120 characters when reasonable
- Use `long` for sizes and indices, not `int` or `size_t`
- Use `uint` (not `unsigned int`) for 32-bit unsigned values

## Testing Pattern

GPU kernel classes have a static `test()` method:
```cpp
// In header (DedispersionKernel.hpp)
class GpuDedispersionKernel {
    static void test();
};

// In implementation (GpuDedispersionKernel.cu)
namespace {  // anonymous namespace for test helpers
    struct TestHelper { ... };
    static void run_test(...) { ... }
}

void GpuDedispersionKernel::test() {
    TestHelper t;
    t.randomize();
    run_test(t);
}
```

Test helpers go in anonymous namespace to avoid cluttering headers.

## Python Interface (pybind11)

Classes are exported in `src_pybind11/pirate_pybind11.cu`:
```cpp
py::class_<GpuDedispersionKernel>(m, "GpuDedispersionKernel")
    .def_static("test", &GpuDedispersionKernel::test);
```

Python CLI: `python -m pirate_frb test --gddk` runs `GpuDedispersionKernel.test()`.
The `--help` flag shows all available test flags.

## CUDA kernels

- Global memory bandwith is usually the most important bottleneck. If possible, ensure that each warp reads/writes entire coalesced cache lines, whenever it accesses global memory.
- Use coalesced, aligned, 64-bit (e.g. float2) or 128-bit (e.g. float4) loads/stores when possible. These instructions can significantly increase global memory bandwidth (compared to 32-bit).
- In float16 kernels, 64-bit and 128-bit loads/stores are awkward, since nvidia doesn't define the appropriate built-in simd type (__half4 or __half8). There are some useful helper functions (device forceinline) in ksgpu/include/ksgpu/device_fp16.hpp.
- **Grid dimension limits**: `gridDim.x` can be up to 2³¹-1, but `gridDim.y` and `gridDim.z` are limited to 65535. Put large dimensions (like nfreq which can be ~10⁵) in `gridDim.x`, not y or z.

## What to do, and not to do

- Please feel free to ask me questions in the chat if my instructions are incomplete
- Don't edit files in `src_lib/autogenerated_kernels/` - they are generated
- Don't add features beyond what's requested
- Don't refactor unrelated code
- Don't create documentation files unless asked
- Don't use `using namespace std` in headers
- When asked to modify `rules.md`, add new rules, but don't delete existing content.
